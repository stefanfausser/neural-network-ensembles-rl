# general parameters
reward_won = 0
reward_lost = 0
reward_draw = 1.0
gamma = 0.95
lambda = 0.5

alpha = 0.001
iterations = 50000
epsilon = 0.96
tau = 0
epsilonOpp = 1.0
tauOpp = 0
stateValueImprecision = 0
decisionNoise = 0
errorStatistics = y

# following parameters are only valid if errorStatistics is "y"
errorEpsilon = 0.000000001
totalErrorIterations = 0
groupErrorIterations = 0
expectedTotalRewardIterations = 0
bellmanErrorIterations = 0

# if defined "y" then the state-values are learned by a MLP
stateValueFunctionApproximation = y

# following parameters are only valid if stateValueFunctionApproximation = n
minA = 0
maxA = 1.0

# following parameters are only valid if stateValueFunctionApproximation = y
linearApproximation = n

# following parameters are only valid if stateValueFunctionApproximation = y and linearApproximation = y
a = 0.4
# trainingMode: 7 = TD
trainingMode = 7

# following parameters are only valid if stateValueFunctionApproximation = y and linearApproximation = n
mlpConfigFile = tetrisMlp.conf
mlpSaveFile = tetris.sav
beta = 0.001
# gradientPrimeFactor is only relevant if RG is used (and not TD)
gradientPrimeFactor = 0
normalizeGradientByAgents = n
normalizeLearningRate = y
tdcOwnSummedGradient = n
replacingTraces = n
# batchSize = 0 means that the weights are trained episode-wise
# and batchSize = X (X > 0) means that the weights are trained after X observed state transitions
batchSize = 1

agents = 1

# following parameters are only used if agents > 1

updateWeightsImmediate = n
weightedDecisions = n
weightedAverage = n
weightCurrentAgent = 0.5
learnFromAverageStateValues = n
averageDecision = n
votingDecision = n
averageDecisionBenchmark = n
votingDecisionBenchmark = n

# seeds for the agents.
# seed = 0 results in a seed generated by the current time

seed1 = 0
seed2 = 0
seed3 = 0
seed4 = 0
seed5 = 0
seed6 = 0
seed7 = 0
seed8 = 0
seed9 = 0
